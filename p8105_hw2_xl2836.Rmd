---
title: "Homework_2"
author: "Xinyi Lin"
date: "9/28/2018"
output: github_document
---

# Problem 1

## Read and clean data

Before dealing with data, we need to load related packages and import data.

```{r}
library(tidyverse)
```

```{r, message = FALSE}
NYC_subway_data = read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")

NYC_subway_data = janitor::clean_names(NYC_subway_data)

head(NYC_subway_data)
```

Then, we try to select the variables we interested and change the structure of the dataset so that it is easier to analyze.

```{r}
NYC_subway_clean_data =
  gather(NYC_subway_data, key = route_number, value = route_name, route1:route11) %>% 
  separate(route_number, into = c("remove", "route_number"), sep = 5) %>%
  select(line, station_name, station_latitude, station_longitude, route_number, route_name, entry, entrance_type, vending, ada) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))

head(NYC_subway_clean_data)
```

In the original dataset, there are `r nrow(NYC_subway_data)` observations and `r ncol(NYC_subway_data)` variables. The variables including division and line of subway, name, latitude and longitude of stations, routes served, staff information and related information about entrance ane exit. 

As `route 1` to `route 11` are same kind of variables, we can combine them to turn them and their values into new variables `route_number` and `route_name`. Besides, we only interested in information about station, routes, entrance and exit, we chose parts of variables for further analysis, including line, station, name, station latitude / longitude, routes served, entry, vending, entrance type and ADA. We also convert the entry variable from character (YES vs NO) to a logical variable. 

Now the new dataset have `r nrow(NYC_subway_clean_data)` observations and `r ncol(NYC_subway_clean_data)` variables and it is tidier.

## Answer questions

### Question 1

There are `r nrow(distinct(NYC_subway_clean_data, line, station_name))` distinct stations which have different station names or lines in the dataset.

### Question 2

```{r}
ada_stations = 
  filter(NYC_subway_data, ada == "TRUE") %>% 
  distinct(station_name, line) %>% 
  nrow()
```

We define that only those stations which with ADA are ADA compliant, in other word, we select stations with entrances/exits that ADA is TRUE. Based on this defination, there are `r ada_stations` stations are ADA compliant.

### Question 3

As the observation in this dataset have either different entrance loation or different entrance type, we define each observation in this dataset as distinct entrance/exit, so the number of entrances/exits without vending is `r nrow(filter(NYC_subway_data, vending == "NO"))` and the number of entrances/exits without vending and allowed entrance is `r nrow(filter(NYC_subway_data, vending == "NO", entry == "YES"))`. The proportion of entrances/exits without vending allow entrance is `r nrow(filter(NYC_subway_data, vending == "NO", entry == "YES"))/nrow(filter(NYC_subway_data, vending == "NO"))`.

### Question 4

```{r}
Atrain_stations =
  filter(NYC_subway_clean_data, route_name == "A") %>% 
  distinct(line, station_name) %>% 
  nrow()
```

```{r}
ada_Atrain_stations =
  filter(NYC_subway_clean_data, route_name == "A", ada == "TRUE") %>% 
  distinct(line, station_name) %>% 
  nrow()
```

There are `r Atrain_stations` distinct stations serve the A train and `r ada_Atrain_stations` of them are ADA compliant.

# Problem 2

## The Mr. Trash Wheel sheet

Read and clean data.

```{r}
library(readxl)
MrTW_data = 
  read_excel(path = "./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", range = "A2:N338") %>% 
  janitor::clean_names() 

head(MrTW_data)

MrTW_clean_data = 
  filter(MrTW_data, !is.na(dumpster)) %>% 
  mutate(sports_balls = as.integer(round(sports_balls)))

head(MrTW_clean_data)
```

## The 2016/2017 precipitation sheet

Read and clean data.

```{r}
precipitation_2016_data = 
  read_excel(path = "./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
                                     sheet = "2016 Precipitation", skip = 1) %>% 
  janitor::clean_names() 

precipitation_2017_data = 
  read_excel(path = "./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
                                     sheet = "2017 Precipitation", skip = 1) %>% 
  janitor::clean_names() 

precip_2016_clean_data = 
  filter(precipitation_2016_data, !is.na(month)) %>% 
  mutate(year = 2016) %>% 
  select(year, month, total)

head(precip_2016_clean_data)

precip_2017_clean_data = 
  filter(precipitation_2017_data, !is.na(month)) %>% 
  mutate(year = 2017) %>% 
  select(year, month, total)

head(precip_2017_clean_data)
```

Combine datasets and convert month to a character variable.

```{r}
precipitation_clean_data = 
  bind_rows(precip_2016_clean_data, precip_2017_clean_data) %>% 
  mutate(month = month.abb[month])

head(precipitation_clean_data)
```

## Overall

```{r}
MrTW_2016_clean_data = filter(MrTW_clean_data, year == "2016") 
sports_balls_median = median(MrTW_2016_clean_data$sports_balls)
```

There are `r nrow(MrTW_clean_data)` observations and `r ncol(MrTW_clean_data)` variables in the `MrTW_clean_data` dataset. The key variables including `dumpster`, `data` and `homes_powered`. There are `r nrow(precipitation_clean_data)` observations and `r ncol(precipitation_clean_data)` variables in the `precipitation_clean_data` dataset. The key variables are `year`,  `month` and `total`. Among them, `r nrow(precip_2016_clean_data)` observations are from 2016, and `r nrow(precip_2017_clean_data)` observations are from 2017. The total precipitation in 2017 is `r sum(precip_2017_clean_data$total)`. The median number of sports balls in a dumpster in 2016 is `r sports_balls_median`.

# Problem 3

## Load and clean data

```{r}
library(p8105.datasets)
data(brfss_smart2010)
```

```{r}
brfss_clean_data = 
  janitor::clean_names(brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% 
  select(-c(class, topic, question, sample_size, confidence_limit_low:geo_location)) %>% 
  reshape::rename(c(locationabbr = "location_abbr", locationdesc = "location_desc"))

brfss_spread_data =
  spread(brfss_clean_data, key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  mutate(high_proportion = excellent + very_good)
```

## Answer questions

### Question 1

```{r}
states_observations = 
  brfss_spread_data$location_abbr %>% 
  as.factor() %>% 
  summary() %>% 
  sort(decreasing = TRUE)

states_observations
```

There are `r nrow(distinct(brfss_spread_data, location_desc))` unique locations included in the dataset. As there are `r nrow(distinct(brfss_spread_data, location_abbr))` distinct states included in the dataset, which equals to the number of states in America plus one (Washington D.C.), so every state is represented in the dataset. By changing the `location_abbr` variable into factor and sorting it, we can find the `r names(states_observations[1])` is observed most.

### Question 2

```{r}
brfss_2002_data = 
  filter(brfss_spread_data, year == 2002) 
```

In 2002, the median of the "Excellent" response value is `r median(brfss_2002_data$excellent, na.rm = TRUE)`.

### Question 3

The histogram of "Excellent" response values in the year 2002 is shown below.

```{r}
ggplot(brfss_2002_data, aes(x = excellent)) +
  geom_histogram(binwidth = 1)
```

### Question 4

The scatterplot showing the proportion of "Excellent" response values in New York County and Queen County in each year from 2002 to 2010 is shown below.

```{r}
brfss_spread_data %>% 
  filter(location_desc == "NY - Queens County" | location_desc == "NY - New York County") %>% 
  ggplot(aes(x = year, y = excellent)) +
  geom_point(aes(color = location_desc))
```

